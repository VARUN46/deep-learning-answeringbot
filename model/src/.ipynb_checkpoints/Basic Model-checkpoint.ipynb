{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30edb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check installed packages version if required\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e85a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dataset\n",
    "dataset = pd.read_csv('datasets/BasicModelSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedDatasetDict = pd.read_csv('datasets/letterEmbedds.csv').set_index('word').T.to_dict()\n",
    "embedDatasetDictVec = pd.read_csv('datasets/letterEmbedds.csv').set_index('embed').T.to_dict()\n",
    "vectorSize = 5\n",
    "#embedDatasetDict['a']['embed']\n",
    "#embedDatasetDictVec[1]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converter functions\n",
    "\n",
    "def convertWordToVector(word):\n",
    "    word = word.lower()[0:vectorSize]\n",
    "    letterArr = [*word]\n",
    "    vectorArr = []\n",
    "    for pos in range(vectorSize):\n",
    "        vectorArr.append(0)\n",
    "    \n",
    "    currentPos = 0\n",
    "    for letter in letterArr:\n",
    "        vectorArr[currentPos] = embedDatasetDict[letter]['embed']\n",
    "        currentPos = currentPos + 1\n",
    "    return vectorArr\n",
    "\n",
    "def convertVectorToWord(vector):\n",
    "    letterArr = []\n",
    "    for vectorVal in vector:\n",
    "        vectorVal = round(vectorVal,0)\n",
    "        if vectorVal != 0:\n",
    "            letterArr.append(embedDatasetDictVec[vectorVal]['word'])\n",
    "    word = ''.join(letterArr)\n",
    "    return word\n",
    "\n",
    "#tests for converter \n",
    "#convertWordToVector('Var')\n",
    "#convertVectorToWord([22, 1, 18, 0, 0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare vector embeddings\n",
    "dataset['InputVectors'] = dataset['Input'].apply(convertWordToVector)\n",
    "dataset['OutputVectors'] = dataset['Output'].apply(convertWordToVector)\n",
    "\n",
    "\n",
    "dataset.to_csv('datasets/datasetWithEmbeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e54c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.convert_to_tensor(dataset['InputVectors'].values.tolist())\n",
    "output_data = tf.convert_to_tensor(dataset['OutputVectors'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=16, activation='relu', input_shape=(5,)),\n",
    "    tf.keras.layers.Dense(units=8, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=5)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_data, output_data, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e51d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('basic_letter_predictor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "64eb18ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ggijl'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#consume the model using below\n",
    "inputData = convertWordToVector('ABCDFF')\n",
    "predictions = model.predict([inputData])\n",
    "convertVectorToWord(predictions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9328fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173b0be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
